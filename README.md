# būk

Grace Bu, CS Senior Project Fall 2023

## Introduction

This repository contains the code for creating and normalizing a database full of book information from [Open Library's data dumps](https://openlibrary.org/developers/dumps), as well as running a web application locally. 

The database and web application were developed for a Senior Project in fulfillment of the Yale B.S. in Computer Science. Keep an eye out for future versions!

To start,  install the dependencies listed in the `requirements.txt` file with the command `pip install -r requirements.txt`.


## The Database <a id="database"></a>

The following are instructions for how to create and normalize what is primarily a database of book information, as the application tables will be filled by interacting with the front end of the application once the book tables are complete. 

To run the application without any book data, it should be possible to create empty tables then run the application. Book and author searches will not be possible, nor will writing reviews or adding books to lists, however.

Please be advised that processing the 72 GB of (uncompressed) data from Open Library may take a significant amount of time, though it should not be difficult with the provided scripts.

### Creating tables

The database schema is defined in `model/database.py` with SQLAlchemy. To create the database, replace the `DB_URL` in `database.py` with your desired PostgreSQL database URL. Then, run `python database.py` to create all the tables in your Postgres database.

### Normalizing Open Library Data

Download the editions, works, and authors dumps from [Open Library's data dumps](https://openlibrary.org/developers/dumps). This will total around 12 GB, but around 72 GB uncompressed. Make sure you have enough disk space! I would recommend having around 150 GB free for loading the database, but once the database is loaded, you can delete all the CSVs to just have a 63 GB database.

The scripts for normalization are found in the `normalization` folder. As each Open Library file is downloaded, you can generate the corresponding CSVs for normalizing each of the major tables (authors, editions, works) by running `normalization/parse_TABLENAME.py`. There are three scripts for this, `parse_author.py`, `parse_editions.py`, and `parse_works.py`. These parsing scripts will parse the raw Open Library data and produce two types of CSVs: one for all non-list fields (one-to-many), and then one file per field of list type (many-to-many). For the major tables, the parsing scripts will also fill in the one-to-one field relationships. 

Once all the CSV files have been generated, run `normalization/normalize.py` to normalize the many-to-many csvs. For example, for the `authors_location.csv` generated by `parse_author.py`, each line will have an author to place mapping. `normalize.py` will find all unique places and create a new csv where each line is (sequence number, place), then replace the places in `authors_location.csv` with its unique sequence number. 

Finally, you can run the commands found in `load_tables.sql` in your `psql` command line table access to load the tables from your CSVs. You should run them all at once, but if there are errors, line-by-line to debug. Replace the file names in `load_tables.sql` with the absolute path to your generated CSV files, as your database may not be in the same directory as the CSVs.

### Indexing the Database

Once the database has been created, be sure that each table has a primary and/or foreign key. Because PostgreSQL does not automatically index foreign keys, for major tables such as `editions_authors` add indexes for both foreign keys. Otherwise, the search will be prohibitively slow.


## The App

[Demo video](https://youtu.be/AXWkf4OLp2k)

### Mission statement

būk is a web application intended to address readers' desires to rate and review books with ease, organize books they've loved into "playlists," and connect with friends over their favorite recent reads.


### Running the app

To run the app, you will need to download and normalize the data first. Please follow the instructions in the [Database section](#database) to do so.

To run the application locally, execute the command `python runserver.py port`, where `port` is the port of your choice, then navigate to the URL displayed in the command line.


### API Documentation

To view the webpages of the API documentation in-app, navigate to the "About" page once logged in and click "API documentation."  
